{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "In the previous notebook we have implemented a dataset that we can now use to access our data. However, in machine learning, we often need to perform a few additional data preparation steps before we can start training models.\n",
    "\n",
    "An important additional class for data preparation is the **DataLoader**. By wrapping a dataset in a dataloader, we will be able to load small subsets of the dataset at a time, instead of having to load each sample separately. In machine learning, the small subsets are referred to as **mini-batches**, which will play an important role later in the lecture.\n",
    "\n",
    "In this notebook, we will implement our own dataloader, which we can then use to load mini-batches from the dataset we implemented previously.\n",
    "\n",
    "First, we need to import libraries and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dl_zero2one.data import DataLoader, DummyDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Iterating over a Dataset\n",
    "Throughout this notebook a dummy dataset will be used that contains all even numbers from 2 to 100. Similar to the dataset we have implemented before, the dummy dataset has a `__len__()` method that allows us to call `len(dataset)`, as well as a `__getitem__()` method, which allows we to call `dataset[i]` and returns a dict `{\"data\": val}` where `val` is the i-th even number. \n",
    "\n",
    "Let's start by defining the dataset, and calling its methods to get a better feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:\t 50 \n",
      "First Element:\t {'data': 2} \n",
      "Last Element:\t {'data': 100}\n"
     ]
    }
   ],
   "source": [
    "dataset = DummyDataset(\n",
    "    root=None,\n",
    "    divisor=2,\n",
    "    limit=100\n",
    ")\n",
    "print(\n",
    "    \"Dataset Length:\\t\", len(dataset),\n",
    "    \"\\nFirst Element:\\t\", dataset[0],\n",
    "    \"\\nLast Element:\\t\", dataset[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will write some code to iterate over the dataset in mini-batches, similarly to what a dataloader is supposed to do. The number of samples to load per mini-batch is called **batch size**. For the remainder of this notebook, the batch size is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a simple function that iterates over the dataset and groups samples into mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batches(dataset, batch_size):\n",
    "    batches = []  # list of all mini-batches\n",
    "    batch = []  # current mini-batch\n",
    "    for i in range(len(dataset)):\n",
    "        batch.append(dataset[i])\n",
    "        if len(batch) == batch_size:  # if the current mini-batch is full,\n",
    "            batches.append(batch)  # add it to the list of mini-batches,\n",
    "            batch = []  # and start a new mini-batch\n",
    "    return batches\n",
    "\n",
    "batches = build_batches(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: [{'data': 2}, {'data': 4}, {'data': 6}]\n",
      "mini-batch 1: [{'data': 8}, {'data': 10}, {'data': 12}]\n",
      "mini-batch 2: [{'data': 14}, {'data': 16}, {'data': 18}]\n",
      "mini-batch 3: [{'data': 20}, {'data': 22}, {'data': 24}]\n",
      "mini-batch 4: [{'data': 26}, {'data': 28}, {'data': 30}]\n",
      "mini-batch 5: [{'data': 32}, {'data': 34}, {'data': 36}]\n",
      "mini-batch 6: [{'data': 38}, {'data': 40}, {'data': 42}]\n",
      "mini-batch 7: [{'data': 44}, {'data': 46}, {'data': 48}]\n",
      "mini-batch 8: [{'data': 50}, {'data': 52}, {'data': 54}]\n",
      "mini-batch 9: [{'data': 56}, {'data': 58}, {'data': 60}]\n",
      "mini-batch 10: [{'data': 62}, {'data': 64}, {'data': 66}]\n",
      "mini-batch 11: [{'data': 68}, {'data': 70}, {'data': 72}]\n",
      "mini-batch 12: [{'data': 74}, {'data': 76}, {'data': 78}]\n",
      "mini-batch 13: [{'data': 80}, {'data': 82}, {'data': 84}]\n",
      "mini-batch 14: [{'data': 86}, {'data': 88}, {'data': 90}]\n",
      "mini-batch 15: [{'data': 92}, {'data': 94}, {'data': 96}]\n"
     ]
    }
   ],
   "source": [
    "def print_batches(batches):  \n",
    "    for i, batch in enumerate(batches):\n",
    "        print(\"mini-batch %d:\" % i, str(batch))\n",
    "\n",
    "print_batches(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the iteration works, but the output is not very pretty. Let us now write a simple function that combines the dictionaries of all samples in a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': [2, 4, 6]}\n",
      "mini-batch 1: {'data': [8, 10, 12]}\n",
      "mini-batch 2: {'data': [14, 16, 18]}\n",
      "mini-batch 3: {'data': [20, 22, 24]}\n",
      "mini-batch 4: {'data': [26, 28, 30]}\n",
      "mini-batch 5: {'data': [32, 34, 36]}\n",
      "mini-batch 6: {'data': [38, 40, 42]}\n",
      "mini-batch 7: {'data': [44, 46, 48]}\n",
      "mini-batch 8: {'data': [50, 52, 54]}\n",
      "mini-batch 9: {'data': [56, 58, 60]}\n",
      "mini-batch 10: {'data': [62, 64, 66]}\n",
      "mini-batch 11: {'data': [68, 70, 72]}\n",
      "mini-batch 12: {'data': [74, 76, 78]}\n",
      "mini-batch 13: {'data': [80, 82, 84]}\n",
      "mini-batch 14: {'data': [86, 88, 90]}\n",
      "mini-batch 15: {'data': [92, 94, 96]}\n"
     ]
    }
   ],
   "source": [
    "def combine_batch_dicts(batch):\n",
    "    batch_dict = {}\n",
    "    for data_dict in batch:\n",
    "        for key, value in data_dict.items():\n",
    "            if key not in batch_dict:\n",
    "                batch_dict[key] = []\n",
    "            batch_dict[key].append(value)\n",
    "    return batch_dict\n",
    "\n",
    "combined_batches = [combine_batch_dicts(batch) for batch in batches]\n",
    "print_batches(combined_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much more organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform operations more efficiently later, we would also like the values of the mini-batches to be contained in a numpy array instead of a simple list. Let's briefly write a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': array([2, 4, 6])}\n",
      "mini-batch 1: {'data': array([ 8, 10, 12])}\n",
      "mini-batch 2: {'data': array([14, 16, 18])}\n",
      "mini-batch 3: {'data': array([20, 22, 24])}\n",
      "mini-batch 4: {'data': array([26, 28, 30])}\n",
      "mini-batch 5: {'data': array([32, 34, 36])}\n",
      "mini-batch 6: {'data': array([38, 40, 42])}\n",
      "mini-batch 7: {'data': array([44, 46, 48])}\n",
      "mini-batch 8: {'data': array([50, 52, 54])}\n",
      "mini-batch 9: {'data': array([56, 58, 60])}\n",
      "mini-batch 10: {'data': array([62, 64, 66])}\n",
      "mini-batch 11: {'data': array([68, 70, 72])}\n",
      "mini-batch 12: {'data': array([74, 76, 78])}\n",
      "mini-batch 13: {'data': array([80, 82, 84])}\n",
      "mini-batch 14: {'data': array([86, 88, 90])}\n",
      "mini-batch 15: {'data': array([92, 94, 96])}\n"
     ]
    }
   ],
   "source": [
    "def batch_to_numpy(batch):\n",
    "    numpy_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        numpy_batch[key] = np.array(value)\n",
    "    return numpy_batch\n",
    "\n",
    "numpy_batches = [batch_to_numpy(batch) for batch in combined_batches]\n",
    "print_batches(numpy_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we would like to make the loading a bit more memory efficient. Instead of loading the entire dataset into memory at once, let us only load samples when they are needed. This can also be done by building a Python generator, using the `yield` keyword. See https://wiki.python.org/moin/Generators for more information on generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': array([ 16,  48, 100])}\n",
      "mini-batch 1: {'data': array([64, 78, 80])}\n",
      "mini-batch 2: {'data': array([46, 22, 72])}\n",
      "mini-batch 3: {'data': array([44, 54, 40])}\n",
      "mini-batch 4: {'data': array([30, 74, 98])}\n",
      "mini-batch 5: {'data': array([24, 84, 20])}\n",
      "mini-batch 6: {'data': array([82, 60, 96])}\n",
      "mini-batch 7: {'data': array([88, 56, 32])}\n",
      "mini-batch 8: {'data': array([62,  8, 90])}\n",
      "mini-batch 9: {'data': array([18, 92, 68])}\n",
      "mini-batch 10: {'data': array([52, 38, 12])}\n",
      "mini-batch 11: {'data': array([26, 94, 70])}\n",
      "mini-batch 12: {'data': array([86, 76, 42])}\n",
      "mini-batch 13: {'data': array([36, 50,  6])}\n",
      "mini-batch 14: {'data': array([10, 34,  2])}\n",
      "mini-batch 15: {'data': array([28, 58, 14])}\n"
     ]
    }
   ],
   "source": [
    "def build_batch_iterator(dataset, batch_size, shuffle):\n",
    "    if shuffle:\n",
    "        index_iterator = iter(np.random.permutation(len(dataset)))  # define indices as iterator\n",
    "    else:\n",
    "        index_iterator = iter(range(len(dataset)))  # define indices as iterator\n",
    "\n",
    "    batch = []\n",
    "    for index in index_iterator:  # iterate over indices using the iterator\n",
    "        batch.append(dataset[index])\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch  # use yield keyword to define a iterable generator\n",
    "            batch = []\n",
    "            \n",
    "batch_iterator = build_batch_iterator(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "batches = []\n",
    "for batch in batch_iterator:\n",
    "    batches.append(batch)\n",
    "\n",
    "print_batches(\n",
    "    [batch_to_numpy(combine_batch_dicts(batch)) for batch in batches]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality of the cell above is now pretty close to what the dataloader is supposed to do. However, there are still two remaining issues:\n",
    "1. The last two samples of the dataset are not contained in any mini-batch. This is because the number of samples in the dataset is not dividable by the batch size, so there are a few left-over samples which are implicitly discarded. Ideally, an option would be prefered that allows us to decide how to handle these last samples.\n",
    "2. The order of the mini-batches, as well as the fact which samples are grouped together, is always in increasing order. Ideally, there should be another option that allows us to randomize which samples are grouped together. The randomization could be easily implemented by randomly permuting the indices of the dataset before iterating over it, e.g. using `indices = np.random.permutation(len(dataset))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DataLoader Class Implementation\n",
    "Now let's put everything together and implement the DataLoader as a proper class.\n",
    "Have a look at the `class DataLoader` of `dl_zero2one/data/image_folder_dataset.py`. \n",
    "\n",
    "\n",
    "Note that the `__init__` method receives four arguments:\n",
    "* **dataset** is the dataset that the dataloader should load.\n",
    "* **batch_size** is the mini-batch size, i.e. the number of samples we want to load at a time.\n",
    "* **shuffle** is binary and defines whether the dataset should be randomly shuffled or not.\n",
    "* **drop_last**: is binary and defines how to handle the last mini-batch in our dataset. Specifically, if the amount of samples in our dataset is not dividable by the mini-batch size, there will be some samples left over in the end. If `drop_last=True`, we simply discard those samples, otherwise we return them together as a smaller mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if our dataloader works as intended. We can change the value of drop_last to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([28, 44, 20])}\n",
      "{'data': array([54, 84, 42])}\n",
      "{'data': array([34, 68, 96])}\n",
      "{'data': array([92, 18, 62])}\n",
      "{'data': array([94, 76, 52])}\n",
      "{'data': array([38,  4, 48])}\n",
      "{'data': array([24, 40,  6])}\n",
      "{'data': array([36, 80, 74])}\n",
      "{'data': array([46, 22, 70])}\n",
      "{'data': array([12, 64, 88])}\n",
      "{'data': array([66, 10, 30])}\n",
      "{'data': array([72,  2, 78])}\n",
      "{'data': array([ 8, 60, 90])}\n",
      "{'data': array([ 98, 100,  56])}\n",
      "{'data': array([16, 58, 86])}\n",
      "{'data': array([82, 14, 26])}\n",
      "{'data': array([32, 50])}\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,    # Change here if you want to see the impact of drop last and check out the last batch\n",
    ")\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "1. In machine learning, we often need to load data in **mini-batches**, which are small subsets of the training dataset. How many samples to load per mini-batch is called the **batch size**.\n",
    "2. In addition to the Dataset class, we use a **DataLoader** class that takes care of mini-batch construction, data shuffling, and more.\n",
    "3. The dataloader is iterable and only loads those samples of the dataset that are needed for the current mini-batch. This can lead to bottlenecks later if we are unable to provide enough batches in time for our upcoming pipeline. This is especially true when loading from HDDs as the slow reading time can be a bottleneck in our complete pipeline later.\n",
    "4. The dataloader task can easily by distributed amongst multiple processes as well as pre-fetched. When we switch to PyTorch later we can directly use our dataset classes and replace our current Dataloader with theirs :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "We have now implemented everything we need to use the CIFAR datasets for deep learning model training. Using our dataset and dataloader, our model training will later look something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DummyDataset(\n",
    "    root=None,\n",
    "    divisor=2,\n",
    "    limit=200,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "model = lambda x: x\n",
    "for minibatch in dataloader:\n",
    "    model_output = model(minibatch)\n",
    "    # do more stuff... (soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
